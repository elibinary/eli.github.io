---
title: 聊聊缓存
date: 2020-10-23 18:48:28
tags:
  - db
description: 本片主要讨论通常业务系统的缓存机制构建问题
---

本片主要讨论通常业务系统的缓存机制构建问题。主要结合 redis、memcached 或其他本地内存缓存组件和数据库场景进行阐述。

在日常开发中，缓存是相当有效的一种提升性能、缓解 DB 压力的方式。在我们大量使用缓存的同时，也有许多细节需要细心处理。

通常开发中使用缓存存储的数据形式可以分成两大类：

* 元数据（如模型层缓存）
* 组合数据或计算结果数据

## 如何保证数据一致性
首先通常在使用缓存时，都会设置合理的过期时间。这在自动过期掉命中率低缓存的同时，还能为数据不一致的情况提供一个兜底时间（最长不一致时间）。

由于缓存主要目的是 cache 住热点数据，提高访问效率并降低 db 压力，**缓存命中率这一指标也是缓存设计及使用的首要关注指标**。

在这种场景下，通常缓存和 db 是两个独立系统（存储），这样不可避免的就会遇到第一个问题：如何保证缓存和 db 的数据一致性？

这其实已经算是一个比较典型的分布式数据一致的场景，在分布式场景下一般保证数据一致性分两大种：最终一致、强一致

### 最终一致
从目的出发，在系统中引入缓存机制其最大目的是为了提高效率，并能够容忍短暂的数据不一致。在这种需求下，我们可以通过操作顺序来实现最终一致，常见做法就是 **Cache Aside**
思路其实非常简单（通常也是我们第一反应的做法）：

1. R 先读缓存，缓存命中直接返回
2. R 读缓存未命中，回源并更新缓存
3. W 写 DB，成功后删除缓存

上面主要的讨论点可能会集中在第三点上：

1. 先删缓存，再写 DB 行不行？
2. 为什么删缓存，而不是写完直接更新缓存？
3. 写 DB 和删缓存是两步操作，删缓存失败怎么办？
4. 并发写入时，导致数据不一致如何解决？
    * W1 写 DB 并清除缓存，记为 data_v1
    * R 读缓存未命中，回源查到 data_v1
    * W2 写 DB 并清除缓存（此时其实缓存已清理），记为 data_v2
    * R 更新 data_v1 到缓存，并设置过期时间
5. 在 Master-Slave 读写分离场景下，数据不一致如何解决？
    * W 写 Master 并清除缓存，记为 data_new
    * R 读缓存未命中，回源读 Slave 得到 data_old（或 null）
    * R 更新 data_old 到缓存，并设置过期时间
    * Slave 同步到 data_new 
    
一个一个来看

**先删缓存，再写 DB 行不行？**
首先第一个问题比较容易理解，如果先删缓存就有可能在 DB 写入前，新的 R 回源更新了旧数据到缓存中。直到下次 W 或者直到缓存过期才会进行更新，从而导致长时间的数据不一致。

**为什么删缓存，而不是写完直接更新缓存？**
为什么不写完直接更新缓存，其实主要从两方面考虑：首先一种情况是缓存存储的可能是聚合数据或者经过计算的结果数据，此时把更新缓存的操作耦合到写操作上并不是一个好的选择；另外可以统一回源入口，仅在一处（R 时）进行回源，这样可以让代码更加易读，也更容易扩展。

**写 DB 和删缓存是两步操作，删缓存失败怎么办？**
第三个问题是非常重要的问题，该场景下的容错处理非常重要。最容易想到的解决方案是增加重试，这种方案的局限性在于如果操作失败是由于执行线程意外中断，或者网络问题导致的丢包超时，短时间连续重试效果并不理想，而递增式重试会长时间 block 当前请求（且存在意外丢失风险）。
虽然通常情况下上面设计已经可以处理绝大多数问题，但能不能进一步提高容错能力呢，答案是肯定的（但同时也是有代价的）。其中有个思路就是通过把清理缓存这一过程异步化来解决本地重试不可靠的问题，下面是一种参考做法思路：

1. 通过引入额外 binlog watch + MQ 来进行异步清理
2. 单线程顺序处理 binlog write 事件，写入 MQ
3. 写 MQ 失败可以使用时间递增式重试
4. Consumer 使用 At-Least-Once 策略消费 MQ 消息，清理缓存

因为要保证 At-Least-Once 的生产/消费，整体系统复杂度又将增加不少，后续对于这一套异步清理的可用性保证、监控等也要增加不少实现复杂度。所以整体设计还是要看业务场景，权衡利弊。

**并发写入时，导致数据不一致如何解决？**
这个问题本质其实是执行顺序问题，存在交叉执行的情况，这种问题最容易想到也是最简单的方法就是加锁。其实本质类似于实现事务的隔离性（单独展开又是一个很大的话题，具体可以参考 DDIA-第七章-事务 一章）。
但是对于缓存场景，其实只要保证缓存中是最新数据就可以了，所以可以选择一种折中做法：通过记录 version 来强制 R 请求回源，每次更新 version 只增不减，大致过程：

1. 在缓存值中增加 version 存储（具体实现可以是两个 key 可以选 hash 结构，也可以暴力直接存储）
2. W 写 DB 后，判断并更新缓存 version 值（比如通过 CAS）
3. R 读缓存对比当前 version 和最新 version，旧则回源更新

不过在选择实现时，请慎重考虑各种方式对并发性能的影响，毕竟应用缓存的首要目标还是提高访问效率。

**在 Master-Slave 读写分离场景下，数据不一致如何解决？**
这种场景下的不一致，主要由于主从同步延迟导致的。解决方式依旧可以参考问题 3、4 的思路。
不同点在于应用监听 binlog 方式时，需要监听的是 Slave 的，也就是 Slave 落库时才清理缓存。而如果使用 version 方式时，需要考虑 Slave 落后这段时间内缓存穿透的问题（由于数据一直是旧的，每次都需要穿透到 Slave 读）。

### 强一致
写在最前面，如果真的有强一致的场景，建议考虑依据具体场景定制的方案，不要使用通用缓存策略。因为无论是单纯引入分布式事务，还是在分布式事务的基础上追加保证高隔离级别（比如引入复杂锁机制），无疑都会极大的影响性能，这与我们引入缓存的目的背道而驰。

## 缓存穿透 & 缓存击穿
这两种场景有些差别：
**缓存穿透**一般在说某个值在 db 中不存在，每一次请求到缓存中那数据拿不到都会回源查 db
最简单解决思路是缓存个空值，这种方式简单有效，但同时也存在缺点：当有很多这种读请求时，将会缓存大量这种数据，在一些恶意攻击中甚至可能会把真正的热点数据挤出缓存。
解决上述问题的一个思路是在读缓存前，再增加一步 check key exist，比如使用 bitmap 或者 bloom filter 快速判断，方便快捷。但同样存在问题：bitmap 或 bloom filter 无论选择存储在本地内存还是存储到 redis 都将面临分布式一致性的问题，同时如果存储到 redis 之类的第三方存储系统，就意味着所有请求都将多一次网络调用。

**缓存击穿**一般在说某个 key 失效了，此时如果不加保护，就会有大量读请求穿透到 DB
一般的解决方案也比较简单，在回源时加互斥锁就可以了，以此保证缓存失效后仅有一个请求触发回源更新，其他请求阻塞等待。

### 缓存雪崩
雪崩是一种特殊场景：当缓存大批量同时失效，或者进一步缓存服务宕机了，此时即使已经进行了击穿保护，依然会有大量请求穿透到 db，增加 db 压力引发 db 宕机风险。解决思路一般从几个方面考虑：

* 首先尽量避免缓存集中失效，比如分散过期时间
* 另一方面保证缓存系统高可用，比如使用集群化解决方案
* 如果整个集群都挂了，那就需要一些兜底方案+限流策略，比如退化使用本地缓存，触发限流丢掉处理不过来的请求保护 DB
* 另外缓存服务刚启起来时，需要进行一段时间缓存预热，此时最好慢慢放开流量

## 总结
在进行方案设计时，应该根据具体场景进行权衡，在实现时免不了要进行各种取舍，适合当前场景的方案才是最好的方案。

