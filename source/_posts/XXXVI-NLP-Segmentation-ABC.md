---
title: 笔记 - 了解分词
date: 2017-08-20 12:26:12
tags:
  - MachineLearning
description: 最近在做的几项工作都需要对文本做预处理，这里就介绍一下常见的分词算法以及好用的实现
---

涉及到搜索、推荐等领域的问题时免不了要和 NPL 打交道，而大部分模型算法第一步予处理阶段就是分词，而中文的语义复杂性，中文分词一直都是一个很重要的方向。

## MMSEG

在介绍 MMSEG 算法之前，首先来看下最简单的分词算法：字典匹配
所谓字典匹配，正如其名字首要要有一个字典，然后在对一段文本进行分词分析的时候参照字典进行分割。
比如我有一个字典，字典中有
```
北京 大学 生产 开发 项目
```
那么在对句子‘北京大学生产开发项目’进行分词的时候就可以很容易得到分词结果了。但单纯的字典匹配会有很多问题，一个很显著的问题就是多匹配。
还是对 ‘北京大学生产开发项目’ 这句话进行分词，假如这是我的词典有
```
北京 大学 北京大学 大学生 生产 开发 项目
```
这时候就会引发问题，会产生
```
北京/大学/生产/开发/项目
北京/大学生/产/开发/项目
北京大学/生产/开发/项目
```
会有多重匹配结果。为了解决这个问题，出现了很多匹配方案，比如经典的最大匹配原则。

MMSEG 算法就是基于字典的中文分词算法，MMSEG 内含有两种算法，分别是 Simple 和 Complex
Simple 即是简单的正向最大匹配，总文本开头正向向后匹配，且以最大匹配为划分。
Complex 则为消除歧义增加分词准确度，而设计了四种规则
1. 最长匹配原则
2. 最大平均长度
3. 最小长度方差
4. 单字词出现频率最高

很显然这是一个基于规则的过程非常直观的分词方法，它把一段文本尽可能长并尽可能均匀的进行划分，这同中文语法习惯比较相符。同时也可以看出，由于 MMSEG 是完全基于词典的，这个引以为根基的字典就尤为重要。选择一个合适完备的字典能够有效提高分词效果。

rmmseg 就是基于 MMSEG 算法的 ruby 实现，最早纯 ruby 编写的 rmmseg 分词非常的慢，而且内存消耗很大。后来作者用 c++ 对其进行了重写。
地址： [rmmseg-cpp][1]

## jieba 分词
jieba 分词也是基于词典的一种分词实现，它基于前缀词典生成句子中汉字所有可能成词情况所构成的有向无环图，以此进行高效的匹配。
而对于词典中不存在的词则采用 HMM 模型，HMM (Hidden Markov Model) 隐马尔科夫模型是一个统计模型，在此处用来标注式的解决没有见过的词，应用非常广泛。

而使用起来也非常简单，下面是 python 的例子
```
import jieba

seg_list = jieba.lcut("我来到北京清华大学", cut_all=True)
# ['我', '来到', '北京', '清华', '清华大学', '华大', '大学']
```

它有三种分词模式，分别是：

* 精确模式，试图将句子最精确地切开，适合文本分析
* 全模式，把句子中所有的可以成词的词语都扫描出来, 速度非常快，但是不能解决歧义
* 搜索引擎模式，在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词

上面 lcut 方法中 cut_all 参数即是用来控制是否采用全模式
```
import jieba

seg_list = jieba.lcut("小明硕士毕业于中国科学院计算所")
# ['小明', '硕士', '毕业', '于', '中国科学院', '计算所']

seg_list = jieba.lcut_for_search("小明硕士毕业于中国科学院计算所")
# ['小明', '硕士', '毕业', '于', '中国', '科学', '学院', '科学院', '中国科学院', '计算', '计算所']
```
lcut_for_search 方法使用搜索引擎模式，进行细粒度划分。

作为分词的核心，jieba 默认使用自带的默认词库，当然你也可以指定自己的词库。而且也支持在程序中动态修改词典，通过使用 
```
add_word(word, freq=None, tag=None)
del_word(word) 
```
方法

不过 jieba 分词暂时没有 ruby 版本

  [1]: https://github.com/pluskid/rmmseg-cpp
